# 爬虫

## request库例子

```python

import requests
req = requests.get("<https://www.baidu.com/>")
print(req.text)
import requests

req = requests.get("<https://www.dart-europe.org/full.php?id=739881>")
print(req.text)
```

## 爬虫例子

### 1

```python

import requests   #导入我们需要的requests功能模块
from bs4 import BeautifulSoup  #使用BeautifulSoup这个功能模块来把充满尖括号的html数据变为更好用的格式，from bs4 import BeautifulSoup这个是说从bs4这个功能模块中导入BeautifulSoup，是的，因为bs4中包含了多个模块，BeautifulSoup只是其中一个
# pip install beautifulsoup4
req = requests.get(url="<https://www.crrcgo.cc/admin/crr_supplier.html?page=1>")  #使用get方式获取该网页的数据。实际上我们获取到的就是浏览器打开百度网址时候首页画面的数据信息
#print(req.text)   #把我们获取数据的文字（text）内容输出（print）出来
req.encoding = "utf-8"  #指定获取的网页内容，即第二句定义req的内容，用utf-8编码
html = req.text   #指定获取的网页内容，即第二句定义req的内容，用text
soup = BeautifulSoup(req.text,features="html.parser")  #用html解析器(parser)来分析我们requests得到的html文字内容，soup就是我们解析出来的结果
company_item = soup.find("div",class_="detail_head")  #find是查找，find_all查找全部。查找标记名是div并且class属性是detail_head的全部元素
dd = company_item.text.strip()   #strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。在这里就是移除多余的尖括号的html数据
print(dd)
```

````纯文本

### 2

```text
import requests
from bs4 import BeautifulSoup

inurl="<https://www.crrcgo.cc/admin/crr_supplier.html?page=>"
for num in range(1,6):
    print("================正在爬虫第"+str(num)+"页数据==================")
    outurl=inurl+str(num)
    req = requests.get(url=outurl)
    req.encoding = "utf-8"
    html=req.text
    soup = BeautifulSoup(req.text,features="html.parser")
    company_items = soup.find_all("div",class_="detail_head")
    for company_item in company_items:
        dd = company_item.text.strip()
        print(dd)
````

### 3

```text
import requests
from bs4 import BeautifulSoup

inurl="<https://www.crrcgo.cc/admin/crr_supplier.html?page=>"
for num in range(1,6):
    print("================正在爬虫第"+str(num)+"页数据==================")
    outurl=inurl+str(num)
    req = requests.get(url=outurl)
    req.encoding = "utf-8"
    html=req.text
    soup = BeautifulSoup(req.text,features="html.parser")
    company_items = soup.find_all("div",class_="detail_head")
    for company_item in company_items:
        dd = company_item.text.strip()
        print(dd)



```

### 获取网页上tr标签的数量

```text
all_tr_tags = soup.find_all('tr')

num_of_tr_tags = len(all_tr_tags)

print("Number of tr tags:", num_of_tr_tags)
```

## Python判断网页标签下面是否有其他子标签

在Python中，可以使用BeautifulSoup库来解析HTML文档，并使用其提供的方法来判断网页标签下面是否有其他子标签。

以下是一个示例代码，演示了如何使用BeautifulSoup库来判断一个`<div>`标签下面是否有其他子标签：

```python
from bs4 import BeautifulSoup

# 假设html_content是包含HTML内容的字符串变量
html_content = """
<html>
<head>
<title>测试页面</title>
</head>
<body>
<div>
  <p>这是一个段落。</p>
  <ul>
    <li>列表项1</li>
    <li>列表项2</li>
  </ul>
</div>
</body>
</html>
"""

# 将HTML内容转换为BeautifulSoup对象
soup = BeautifulSoup(html_content, 'html.parser')

# 查找目标标签
target_tag = soup.find('div')

# 判断目标标签下面是否有其他子标签
if target_tag and target_tag.contents:
    print("该标签下面有其他子标签")
else:
    print("该标签下面没有其他子标签")
```

在上面的代码中，首先将HTML内容转换为BeautifulSoup对象，然后使用`find()`方法查找目标标签。

使用`contents`属性获取目标标签的所有子节点，如果该属性返回的结果不为空，则说明目标标签下面有其他子标签；否则，说明目标标签下面没有其他子标签。

## Python对连接进行下载

在Python中下载链接，可以使用`urllib`或`requests`库来实现。以下是使用`urllib`库下载链接python import urllib.request

url = "[https://www.example.com/image.jpg](https://link.zhihu.com/?target=https%3A//www.example.com/image.jpg "https://www.example.com/image.jpg")"

# 要下载的链接

### 使用urllib.request.urlretrieve()方法下载链接

urllib.request.urlretrieve(url, "image.jpg") # 将链接内容保存为image.jpg文件

Copy 在上面的示例中，我们使用`urllib.request.urlretrieve()`来下载链接，并将内容保存为名为`image.jpg`的文件。

如果你想使用`requests`库来下载链接，请使用以下示例代码：

```python
import requests

url = "<https://www.example.com/image.jpg>"  # 要下载的链接

# 发送GET请求并获取链接内容
response = requests.get(url)

# 将内容保存为image.jpg文件
with open("image.jpg", "wb") as file:
    file.write(response.content)
```

在上面的示例中，我们使用`requests.get()`方法发送GET请求并获取链接内容，然后使用`open()`函数将内容保存为名为`image.jpg`的文件。

无论你选择使用`urllib`还是`requests`，都可以根据需要进行适当的错误处理和进一步的文件操作。

### pdf

要下载PDF文件，可以使用相同的库 (`urllib` 或 `requests`) 进行下载。以下是针对 PDF 文件下载的示例代码：

使用 `urllib` 库下载 PDF 文件：

```python
import urllib.request

url = "<https://www.example.com/example.pdf>"  # 要下载的 PDF 文件链接
urllib.request.urlretrieve(url, "example.pdf")  # 将链接内容保存为 example.pdf 文件
```

在上述示例中，我们使用 `urllib.request.urlretrieve()` 方法下载链接，并将内容保存为名为 `example.pdf` 的文件。

使用 `requests` 库下载 PDF 文件：

```python
import requests

url = "<https://www.example.com/example.pdf>"  # 要下载的 PDF 文件链接
response = requests.get(url)  # 发送 GET 请求并获取链接内容
with open("example.pdf", "wb") as file:
    file.write(response.content)  # 将内容保存为 example.pdf 文件
```

在上述示例中，我们使用 `requests.get()` 方法发送 GET 请求并获取链接内容，然后使用 `open()` 函数将内容保存为名为 `example.pdf` 的文件。

无论你选择使用 `urllib` 还是 `requests`，都可以根据需要进行适当的错误处理和进一步的文件操作。

```python
import requests

# 有空格
url = "<http://etheses.whiterose.ac.uk/6879/1/JRL-Agustinus%20Sutiono-Thesis-20140101-01.pdf>"  # 要下载的 PDF 文件链接
url_text = url.replace(" ", "%20")
print(url_text)

response = requests.get(url_text)  # 发送 GET 请求并获取链接内容
with open("./pdf/example.pdf", "wb") as file:
    file.write(response.content)  # 将内容保存为 example.pdf 文件
```
